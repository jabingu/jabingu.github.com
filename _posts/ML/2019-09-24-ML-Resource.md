---
layout: post
title:  "机器学习：网上资料整合"
date:   2019-09-24 6:30:00
categories: ML
tags: ML 
thumbnail: "assets/img/post/ai.jpg"
feature-img: "assets/img/post/ai.jpg"
excerpt: 机器学习：网上资料整合
---

* content
{:toc}
> 使用机器学习来学习人类，人类再去学习机器学习的结果。
>
> <p align="right">—-jabingu　　</p>



> 目前还缺少的是新的方法:比如gen网络，注意力机制等等。之后把这些进行整理。



## 视频教材

[机器学习（Machine Learning）- 吴恩达（Andrew Ng）](https://www.bilibili.com/video/av9912938?from=search&seid=10310828645516476483)



## 资源收集

[机器学习(周志华西瓜书)参考答案总目录](https://blog.csdn.net/icefire_tyh/article/details/52064910)

[[机器学习入门] 经典台大李宏毅机器学习课程从这里开始](https://blog.csdn.net/soulmeetliang/article/details/77461607)





[机器学习资源收集、索引](https://www.cnblogs.com/dyntao/p/8648969.html)

[机器学习 人工智能 博文链接汇总](https://blog.csdn.net/aliceyangxi1987/article/details/71911003)



## 机器学习实战

### 个人网站：Jack Cui

[机器学习实战教程（一）：K-近邻算法（史诗级干货长文）](https://cuijiahua.com/blog/2017/11/ml_1_knn.html)

[机器学习实战教程（二）：决策树基础篇之让我们从相亲说起](https://cuijiahua.com/blog/2017/11/ml_2_decision_tree_1.html)

[机器学习实战教程（三）：决策树实战篇之为自己配个隐形眼镜](https://cuijiahua.com/blog/2017/11/ml_3_decision_tree_2.html)

[机器学习实战教程（四）：朴素贝叶斯基础篇之言论过滤器](https://cuijiahua.com/blog/2017/11/ml_4_bayes_1.html)

[机器学习实战教程（五）：朴素贝叶斯实战篇之新浪新闻分类](https://cuijiahua.com/blog/2017/11/ml_5_bayes_2.html)

[机器学习实战教程（六）：Logistic回归基础篇之梯度上升算法](https://cuijiahua.com/blog/2017/11/ml_6_logistic_1.html)

[机器学习实战教程（七）：Logistic回归实战篇之预测病马死亡率](https://cuijiahua.com/blog/2017/11/ml_7_logistic_2.html)

[机器学习实战教程（八）：支持向量机原理篇之手撕线性SVM](https://cuijiahua.com/blog/2017/11/ml_8_svm_1.html)

[机器学习实战教程（九）：支持向量机实战篇之再撕非线性SVM](https://cuijiahua.com/blog/2017/11/ml_9_svm_2.html)

[机器学习实战教程（十）：提升分类器性能利器-AdaBoost](https://cuijiahua.com/blog/2017/11/ml_10_adaboost.html)

[机器学习实战教程（十一）：线性回归基础篇之预测鲍鱼年龄](https://cuijiahua.com/blog/2017/11/ml_11_regression_1.html)

[机器学习实战教程（十二）：线性回归提高篇之乐高玩具套件二手价预测](https://cuijiahua.com/blog/2017/12/ml_12_regression_2.html)

[机器学习实战教程（十三）：树回归基础篇之CART算法与树剪枝](https://cuijiahua.com/blog/2017/12/ml_13_regtree_1.html)


## 深度学习

###  公众号：搜索与推荐Wiki

[常见的五种神经网络(1)-前馈神经网络](https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=2247485365&idx=1&sn=6599f17f954968b2225b4ba9ef060bd3&chksm=ea66cf2fdd1146394a3f377af9773d9d6eda02149dc8b2fa93e6dd0843c323f05f0f9eafc50f&scene=21#wechat_redirect)

[常见的五种神经网络(2)-卷积神经网络](https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=2247485598&idx=1&sn=3a7ff3c6d5a9dc3d88bc325f714d9f9b&chksm=ea66c004dd11491204297b1c87d02cee86fded584e96c2a77a861949c75160977e5731aa866c&scene=21#wechat_redirect)

[常见的五种神经网络(3)-循环神经网络（上）篇](https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=2247485661&idx=1&sn=aeaba5397ba00f1bf7d379cb07e1ac69&chksm=ea66c047dd1149516c49888a7a13cec5b5e2185aee481b078e6d1c060d2acec89e275fe90c40&scene=21#wechat_redirect)

[常见的五种神经网络(3)-循环神经网络（下）篇](https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=2247485860&idx=1&sn=99fb91b1932374ab3c3812e261a481bb&chksm=ea66c13edd1148287fd4013d87d0abe6b25932f145199f2e1b79e2c2e4947ca5ffaceaeda8f3&scene=21#wechat_redirect)



### @hanbingtao

[零基础入门深度学习(1) - 感知器](https://www.zybuluo.com/hanbingtao/note/433855)

[零基础入门深度学习(2) - 线性单元和梯度下降](https://www.zybuluo.com/hanbingtao/note/448086)

[零基础入门深度学习(3) - 神经网络和反向传播算法](https://www.zybuluo.com/hanbingtao/note/476663)

[零基础入门深度学习(4) - 卷积神经网络](https://www.zybuluo.com/hanbingtao/note/485480)

[零基础入门深度学习(5) - 循环神经网络](https://zybuluo.com/hanbingtao/note/541458)

[零基础入门深度学习(6) - 长短时记忆网络(LSTM)](https://zybuluo.com/hanbingtao/note/581764)

[零基础入门深度学习(7) - 递归神经网络](https://zybuluo.com/hanbingtao/note/626300)



### 公众号：远洋号

[2018汇总深度学习篇](https://mp.weixin.qq.com/s?__biz=MjM5MzA1Mzc3Nw==&mid=2247484912&idx=1&sn=e073c55b0ed6bd838df88af4913e68af&chksm=a69da8ce91ea21d87253c5c3fe92453c176d10fe83c3e4333c06fc5f720cd9285a452f657bdb&scene=21#wechat_redirect)



## 优化函数

### 公众号：搜索与推荐Wiki

[神经网络中的网络优化和正则化（一）之学习率衰减和动态梯度方向](https://mp.weixin.qq.com/s/wwV2ZL1dQ9duVWAL_Dugsw)

[神经网络中的网络优化和正则化（二）之参数初始化/数据预处理/逐层归一化](https://mp.weixin.qq.com/s?src=11&timestamp=1569376689&ver=1873&signature=QT*56qzOaGnqaVAurUgA7Rltrk6BWwSCwXUv0UDfqWExDamCmyRShyZq7DV8MdbOg-4dkq7u-hyrrWqPIKMdxs2IaRv0i*hxhjp95aUKSsXWmPtR4tiscfMG9gd8WZ62&new=1)



###  CSDN:史丹利复合田

[深度学习优化函数详解（0）-- 线性回归问题](https://blog.csdn.net/tsyccnh/article/details/75714791)

[深度学习优化函数详解（1）-- Gradient Descent 梯度下降法](https://blog.csdn.net/tsyccnh/article/details/75948021)

[深度学习优化函数详解（2）-- SGD 随机梯度下降](https://blog.csdn.net/tsyccnh/article/details/76064087)

[深度学习优化函数详解（3）-- mini-batch SGD 小批量随机梯度下降](https://blog.csdn.net/tsyccnh/article/details/76136771)

[深度学习优化函数详解（4）-- momentum 动量法](https://blog.csdn.net/tsyccnh/article/details/76270707)

[深度学习优化函数详解（5）-- Nesterov accelerated gradient (NAG)](https://blog.csdn.net/tsyccnh/article/details/76673073)

[深度学习优化函数详解（6）-- adagrad](https://blog.csdn.net/tsyccnh/article/details/76769232)





