---
layout: post
title:  "ICCD 2017"
date:   2019-07-02 7:30:00
categories: conference
tags: conference ICCD
excerpt: 记录ICCD 2017会议文章的摘要和主要内容，方便日后的检索。
---

* content
{:toc}
> 死亡是活过的生命，生活是在路上的死亡。
>
> <p align="right">——博尔赫斯　　</p>



**目录：**

> **Best Papers Session    
>
> Session 1A: Hardware Security I          
>
> Session 1B: Read-Write Optimizations for Non-Volatile Memory    
>
> Session 2A: Stochastic, Approximate, and Unary Computing
>
> Session 2B: Energy-Efficiency through Heterogeneity
>
> Session 3A: Debugging and Validation
>
> Session 3B: Graph Processing and NoC Architectures
>
> Session 4A: EDA with Focus on Multicore, FPGAs, and 3D
>
> Session 4B: Hardware Acceleration for Neural Networks
>
> Session 5A: Hardware Security II
>
> Session 5B: Memory and Cache Optimizations
>
> Session 6A: Verification and Fault Tolerance
>
> Session 6B: Lithography and Patterning
>
> Special Session 1: On How to Design and Manage Complex Heterogeneous Distributed Computing Systems
>
> Session 7A: LCD with Focus on Emerging Technology
>
> Session 7B: Power-Performance Optimization of Multicore Architecture
>
> Session 8A: Synthesis and Security
>
> Session 8B: Cloud and Storage Solutions
>
> Special Session 2: Effective Voltage Scaling in Late CMOS Era
>
> Special Session 3: Spin-Computing: Lower the Barrier between Memory and Logic
>
> Session 9A: Architecture and Microarchitecture Optimizations
>
> Session 9B: Novel Architecture with 3D and Flash Memory







## Best Papers Session

## S1A: Hardware Security I

## S1B: Read-Write Optimizations for Non-Volatile Memory

**Adaptive Prefetching for Accelerating Read and Write in NVM-Based File Systems**

> ​	Shengan Zheng, Hong Mei, Linpeng Huang, Yanyan Shen, Yanmin Zhu:

The byte-addressable Non-Volatile Memory (**NVM**) offers fast, fine-grained access to persistent storage. While DRAM and NVM have similar read performance, the write operations of existing NVM materials incur longer latency and lower bandwidth than DRAM. This read-write asymmetry nature of NVM causes two bottlenecks for **accessing read-and write-intensive file data**: <u>expensive data block lookups via file inner structure</u> and <u>high-latency direct writes to data blocks in NVM</u>. However, existing NVM-based file systems fail to address both bottlenecks well. This paper presents **WARP**, an **adaptive prefetching module** designed for NVM-based file systems, which aims to deal with two bottlenecks effectively. WARP employs two acceleration approaches: 1) <u>mapping data blocks into kernel virtual address space to bypass the indirection of file inner structure for read-intensive file data</u>; and 2) <u>allocating DRAM buffer to absorb frequent writes for write-intensive file data</u>. We design a WARP benefit model to identify read-and write-intensive access patterns for file data, and use a successor prediction model to predict future data access based on historical file access traces. With WARP, we are able to prefetch file data according to both file access patterns and traces with consistency guarantee. WARP can be implemented on various NVM-based file systems, and we choose HMVFS for the experiments. The evaluation results show that HMVFS with WARP provides high prefetching accuracy and up to 32%-83% improvement compared with the state-of-the-art NVM-based **file systems**.

**NVM-based file system**；**预取**；**buffer**

----

**A Cost-Efficient NVM-Based Journaling Scheme for File Systems.** 

> ​	Xiaoyi Zhang, Dan Feng, Yu Hua, Jianxi Chen

Modern file systems employ **journaling** techniques to guarantee **data consistency** in case of unexpected system crashes or power failures. However, journaling file systems usually suffer from performance decrease <u>due to the extra journal writes</u>. Moreover, the emerging non-volatile memory technologies (**NVMs**) have the potential capability to improve the performance of journaling file systems by being deployed as the journaling storage devices. However, traditional journaling techniques, which are designed for hard disks, fail to perform efficiently in NVMs. In order to address this problem, we propose an **NVM-based journaling scheme**, called **NJS**. The basic idea behind NJS is to <u>reduce the journaling overhead of traditional file systems while fully exploiting the byte-accessibility characteristic</u>, and <u>alleviating the relatively slow write and endurance limitation of NVM</u>. Our NJS consists of three major contributions: (i) <u>In order to minimize the amount of journal writes</u>, NJS only needs to write the metadata of file systems and over-write data to NVM as write-ahead logging, thus alleviating the relatively slow write and endurance limitation of NVM. (ii) We propose <u>a novel journaling update scheme</u> in which the journaling data blocks can be updated in the byte-granularity based on the difference of the old and new versions of journal blocks, thus fully exploiting the unique byte-accessibility characteristic of NVM. (iii) NJS includes <u>a garbage collection mechanism</u> that absorbs the redundant journal updates, and actively delays the checkpointing to the file system. Evaluation results show the efficiency and efficacy of NJS. For example, compared with original Ext4 with a ramdisk-based journaling device, the throughput improvement of Ext4 with our NJS is up to 137.1%.

**NVM-Based Journaling Scheme**；

----

**TDV Cache: Organizing Off-Chip DRAM Cache of NVMM from a Fusion Perspective.** 

> ​	Tianyue Lu, Yuhang Liu, Haiyang Pan, Mingyu Chen

Emerging Non-Volatile Memory (**NVM**) provides both larger memory capacity and higher energy efficiency, but has much longer access latency than traditional DRAM, thus DRAM can be used as an efficient cache to hide the long latency of Non-Volatile Main Memory (NVMM) system. **Transparent Off-chip DRAM cache** (**TOD cache**) is a new DRAM cache structure where off-chip DRAM module is used as **L4 cache** and managed by hardware. The capacity and latency ratio of TOD cache over NVM are both quite different from those of traditional on-chip SRAM or die-stacked DRAM cache over off-chip DRAM memory. All the factors including hit latency, miss latency and hit rate need to be re-considered for TOD cache design. In this study, we first point out that three types of traditional cache schemes cannot be used directly for TOD cache, since set-associative cache suffers from extra tag lookup latency, direct-mapped cache has low hit rate and tag cache is too small to efficiently hold the working sets of tags for DRAM cache. Based on these observations, we propose a novel cache scheme, **TDV**, that fuses these three different types of cache together to take their advantages. In TDV, a direct-mapped cache is used as the first-level cache to achieve short access latency, a set-associative victim cache is taken as the second-level cache to obtain extra high hit rate, and a SRAM tag cache only serves for the victim cache rather than the whole DRAM cache and thus improves the hit rate of tag cache significantly. The simulation results show that, TDV cache has a performance improvement of 6.3% and 8.3% on average than state-of-the-art direct-mapped (Alloy cache) and set-associative cache (ATCache) with same DRAM and SRAM capacity.

L4 cache

----

**RCTP: Region Correlated Temporal Prefetcher**

>   Dennis Antony Varkey, Biswabandan Panda, Madhu Mutyam:

**Hardware prefetcher** is an essential component of modern processors that helps in boosting system performance by fetching the data before processor demands for the same. Hardware prefetching techniques have been proposed to exploit various kinds of access patterns. However, there are applications that are highly irregular in nature that evolved in the past decade, and have massive memory footprint. Temporal prefetching techniques are effective in predicting the future addresses of these irregular applications. Prior works on temporal prefetching use large data structures to store temporal patterns and future memory accesses are predicted using these patterns. <u>However, these techniques predict future accesses only when there is a pattern that is already trained for a given cache line address</u>. To address this issue, we propose **Region Correlated Temporal Prefetcher** (**RCTP**). Our technique correlates temporal patterns of memory regions and predicts future accesses for the region whose patterns are yet to be populated. Thus, RCTP helps in predicting cache line addresses whose first access is yet to happen unlike the traditional temporal prefetchers. We evaluate RCTP on SPEC CPU 2006, CRONO, and PBBS benchmark suites. RCTP outperforms the state-of-the-art temporal prefetcher named ISB by 26%, and a recent delta prefetcher called VLDP by 6%. This improvement comes with a hardware overhead of 1KB over ISB; however, RCTP does not require off-chip storage, unlike other temporal prefetchers.

**硬件预取**；时间预取器；

----

**A Shingle-Aware Persistent Cache Management Scheme for DM-SMR Disks.**

> ​	Tianming Yang, Haitao Wu, Ping Huang, Fei Zhang:

**Shingled Magnetic Recording** (**SMR**) Technology is a new promising **disk** technology, which enables high areal density for disk storage by adopting writing tracks in an overlapped manner. Due to track overlapping, however, SMR devices cannot support in-place updates on shingled disk tracks, as overwriting shingled tracks would damage previously written data on neighboring tracks, resulting in poor random write performance. **Drive-managed SMR** (**DM-SMR**) devices attempt to circumvent this idiosyncrasy by deploying an internal persistent cache which absorbs incoming writes temporally and persists them later on in batches, while providing backward compatibility by reserving the same block access interface. The persistent cache management policy has to maintain the mapping associations between fresh writes in the persistent cache and their target destinations on the disk, resulting in a so-called shingle translation layer (STL). Leveraging the sequential-only write property of shingled devices, in this paper, we propose <u>a new shingle aware persistent cache cleaning policy</u> for DM-SMR drives. Unlike traditional management polices, our new policy first merges cached updates by flushing writes that can be safely written to the disk, i.e., in the shingle direction, so that the cache space can be freed without paying the cost of read-modify-write. It then defaults to the normal cache merging process if it needs to reclaim more cache space. Our evaluations have shown that our persistent cache management policy delivers better performance (by up to 2.5X) via dramatically reducing write amplification associated with persistent cache cleaning and alleviating fragmented reads.

**磁盘SMR**

## S2A: Stochastic, Approximate, and Unary Computing

## S2B: Energy-Efficiency through Heterogeneity

## S3A: Debugging and Validation

## S3B: Graph Processing and NoC Architectures

## S4A: EDA with Focus on Multicore, FPGAs, and 3D

### S4B: Hardware Acceleration for Neural Networks

## S5A: Hardware Security II

## S5B: Memory and Cache Optimizations

**DAAIP: Deadblock Aware Adaptive Insertion Policy for High Performance Caching.**

> ​	Newton, Sujit Kr Mahto, Suhit Pai, Virendra Singh:

The commonly used **LRU** replacement policy for management of shared last-level cache (LLC) is not efficient as the policy is sharing-oblivious. LRU policy is suitable for applications which show a high-degree of data locality i.e. applications which are cache-friendly. However, applications with working data set greater than the available cache size or poor temporal locality perform poorly with LRU as most of the cache lines inserted by them simply traverse from MRU to LRU position without being re-referenced. Such applications are streaming in their cache behaviour and have very less data reuse. LRU policy makes inefficient use of **shared caches** for application mixes which are a combination of cache-friendly and streaming applications as the policy treats each cache line independently and doesn't learn from application's past cache reuse behaviour. We show that simple adaptive changes to the insertion policy can significantly improve system's performance. We propose **Deadblock Aware Adaptive Insertion Policy** (**DAAIP**) which dynamically adapts to the changing cache behaviour of applications sharing the LLC. DAAIP protects the data of application having high temporal locality from high access rate thrashing/streaming applications. Our proposed mechanism monitors each application at-runtime using cost-effective hardware circuits. The information collected is used to dynamically modify the insertion policy and implicitly partition the cache in favour of application showing more data locality. Our evaluation, with 39 multiprogrammed workloads, shows that DAAIP improves performance of dual-core system by up to 21% and on an average 5.8% over LLC caches managed by SRRIP replacement policy. We show that DAAIP also outperforms state-of-the-art cache replacement policy ABRip by 4.6% on system throughput metric.

**cache替换策略**

----

**Dual Dictionary Compression for the Last Level Cache.**

> ​	Akshay Lahiry, David R. Kaeli:

The performance of **GPUs** is rapidly improving as the top GPU vendors keep pushing the boundaries of process technologies. While larger die sizes help improve performance given the nature of parallel workloads, additional architectural improvements can also help by utilizing the available die real estate more efficiently. Introducing a compressed **Last Level Cache** (**LLC**) can make better use of die area, and can improve memory system performance. With widespread adoption of high-resolution displays, most modern game developers are trying to generate high quality graphics output leveraging state-of-the-art GPUs, all of which greatly increases amount of data that needs to be processed. These modern graphics workloads will need to rely on compression to help save memory bandwidth and improve the performance of the LLC. **A compressed LLC** can help by increasing the hit-rate due to logical cache expansion, as well as provide bandwidth savings due to compressed data on the memory bus. In this paper we propose a novel scheme to extend <u>dynamic dictionary-based compression</u> to store compressed data in memory. Current dictionary-based compression schemes need to decompress the data when a cache block gets evicted. This is because the dynamic dictionary entries are not guaranteed to stay the same and data consistency cannot be maintained. This results in bandwidth savings that is limited to the logical cache expansion. We propose a dual-dictionary scheme (DDC) that can help maintain data consistency, as well as improve bandwidth savings. Our scheme saves bandwidth by coupling logical cache expansion with compressed data on the memory bus. We achieve bandwidth savings of 18.55% for reads and 11.01% for writes, on average, for a diverse range of graphics workloads.

**compression**

----

**Jenga: Efficient Fault Tolerance for Stacked DRAM**

> ​	Georgios Mappouras, Alireza Vahid, A. Robert Calderbank, Derek R. Hower, Daniel J. Sorin:

In this paper, we introduce **Jenga**, a new scheme for protecting 3D DRAM, specifically high bandwidth memory (HBM), from failures in bits, rows, banks, channels, dies, and TSVs. By providing redundancy at the granularity of a cache block-rather than across blocks, as in the current state of the art-Jenga achieves greater error-free performance and lower error recovery latency. We show that Jenga's runtime is on average only 1.03x the runtime of our Baseline across a range of benchmarks. Additionally, for memory intensive benchmarks, Jenga is on average 1.11x faster than prior work.

----

**SelSMaP: A Selective Stride Masking Prefetching Scheme.**

> Jiajun Wang, Reena Panda, Lizy Kurian John:

Although **prefetching** concepts have been proposed for decades, new challenges are introduced by sophisticated system architecture and emerging applications. Large instruction windows coupled with out-of-order execution makes program data access sequence distorted from cache perspective. Big data applications stress memory subsystems heavily with their large working set sizes and complex data access patterns. To address such challenges, this work proposes <u>a high performance hardware prefetching scheme</u>, **SelSMaP**. SelSMaP is able to detect both <u>regular</u> and <u>non-uniform stride</u> patterns by <u>taking the minimum observed address offset</u> (called a **reference stride**) as a heuristic. We evaluated SelSMaP with CloudSuite workloads and SPEC CPU2006 benchmarks. SelSMaP achieves an average CloudSuite performance improvement of 30% over non-prefetching system. With one to two order of magnitude less storage and much less functional logic, SelSMaP outperforms the highest-performing prefetcher by 8.6% in CloudSuite workloads.

**片上预取**

----

**T2: A Highly Accurate and Energy Efficient Stride Prefetcher.**

> Sushant Kondguli, Michael Huang:

**Prefetching** is a central element in most microarchitectures. Many different algorithms have been proposed with varying degrees of complexity and effectiveness. There is a tradeoff among various aspects of <u>coverage</u>, <u>accuracy</u>, and <u>cost</u>, especially when we try to exploit both simpler access patterns and more complex ones simultaneously. In this paper, we propose a design that only targets **canonical strided** access patterns, but does so with a very high accuracy. Compared to many other state-of-the-art prefetchers, sometimes with much more ambitious coverage, our design incurs much less memory traffic, reduces energy consumption, while still performs better on average.

## S6A: Verification and Fault Tolerance

## S6B: Lithography and Patterning

## SS1: On How to Design and Manage Complex Heterogeneous Distributed Computing Systems

## S7A: LCD with Focus on Emerging Technology

## S7B: Power-Performance Optimization of Multicore Architecture

## S8A: Synthesis and Security

## S8B: Cloud and Storage Solutions

**A Scale-Out Enterprise Storage Architecture.** 

> Wonil Choi, Myoungsoo Jung, Mahmut T. Kandemir, Chita R. Das:

A robust **enterprise SSD** design should provide scalable throughput and storage capacity by integrating (up to thousands) flash chips in a scale-out fashion. However, the current "channel-based" SSD architecture is not a scalable design choice to allow such a dense integration. Motivated by the inherent architectural scalability of PCIe, we propose **UT-SSD**, <u>a novel enterprise-scale scale-out SSD architecture</u>, which enables the connection of a large number of (1000s) flash chips using the native PCIe buses instead of the conventional channels. We also propose an architectural enhancement that further improves the performance of our base UT-SSD by maximizing flash utilization. Our experimental analysis of UT-SSD with workloads drawn from various domains shows that the throughput of UT-SSD can reach up to 110 GB/s by successfully aggregating the bandwidth of 4096 flash chips. In addition, our proposed enhancement over this base UT-SSD increases the flash utilization by 50.7%, which in turn results in 116% additional throughput improvement.

**enterprise SSD**

----

**CloudShelter: Protecting Virtual Machines' Memory Resource Availability in Clouds.** 

> ​	Tianwei Zhang, Yuan Xu, Yungang Bao, Ruby B. Lee:

We present **CloudShelter**, an architecture to protect virtual machines' memory availability from undesired resource contention on the **cloud** servers. We introduce a new micro-architectural metric: **Memory Round Trip Time**, to quantify VMs' memory QoS. Using this metric, (1) CloudShelter defines new QoS options for customers when launching VMs. These options can guarantee VMs' memory QoS at different levels even when they face intensive contention with co-located VMs; (2) CloudShelter periodically monitors VMs' memory QoS at runtime: once QoS violations against customers' demands are detected, CloudShelter places this VM into an isolated environment to eliminate contention. CloudShelter can reduce 30.1% performance interference from LLC/DRAM contention and 81.6% interference from bus contention1.

cloud

----

**Using Disturbance Compensation and Data Clustering (DC)2 to Improve Reliability and Performance of 3D MLC Flash Memory.**

> Yazhi Feng, Dan Feng, Wei Tong, Yu Jiang, Chuanqi Liu

**3D architectures** are considered the most promising approach to continuously increasing memory density and reducing cost/bit for NAND flash memory by stacking more layers. However, 3D MLC flash memory brings two serious problems, referred to as cell-to-cell program disturbance and big block problem. To solve the disturbance problem for better reliability, we proposed a <u>Disturbance Compensation Programming Scheme</u> (**DCPS**). Based on quantitatively analyzing the disturbance from each direction in 3D flash memory, the scheme accordingly set the **verify voltage** (**VVFY**) a little lower than the original value when **Incremental Step Pulse Programming** (**ISPP**) is performed. After disturbance compensation, the threshold voltage of flash cells can shift towards the ideal distribution. Moreover, **Read reference Voltage Shifting** (**RVS**) and **Artificial Compensation** (**AC**) strategies on margin pages are introduced to adapt to the three-dimensional structures to further enhance reliability. To solve big block problem, **Multiple-Level-Queue page** allocation (**MLQ**) is proposed. We use multiple queues to filter the logical addresses of different update counts and choose different data blocks to respond. The stored data are gradually well organized and generate less data migration when performing garbage collection. Experimental results show that our design reduces the disturbed BER by at least 82% with respect to a FTL with traditional allocation and garbage collection scheme. Besides, we demonstrate MLQ can achieve more effective results than the state of the art scheme in the big block environment. The write amplification, I/O response time and the number of erasures are reduced by 31.2%, 22.2% and 14.6% on average, respectively.

Read reference Voltage Shifting；减少BER

----

**Improving Performance of TLC RRAM with Compression-Ratio-Aware Data Encoding.**

> ​	Jie Xu, Dan Feng, Wei Tong, Jingning Liu, Wen Zhou:

Resistive Random Access Memory (**RRAM**) technology is proposed as a promising replacement candidate for DRAM-based main memory due to its good scalability, low standby power, and non-volatility. The structure of Triple-Level Cell (TLC) can offer higher data density over Single-Level Cell (SLC). However, TLC RRAM suffers from high write energy and latency. **Data compression** techniques can reduce the size of the data to store. In contrast, data encoding methods such as **Incomplete Data Mapping** (**IDM**) can 'expand' the size for latency and energy reduction. We observe that the compression ratio of each cacheline varies, and therefore the saved space of each compressed cacheline is different. On the other hand, we find that different IDMs have different tradeoffs in capacity and write latency/energy. To fully exploit the space saved by compression for reducing the write latency/energy, and improving the performance of TLC RRAM-based main memory system, **Compression-Ratio-Aware Data Encoding** (**CRADE**) is proposed. The key idea of CRADE is to **dynamically** <u>select the best-performing IDM according to the compression ratio of each cacheline</u>. The cacheline is compressed first, and then the compressed cacheline is encoded by IDM. For each compressed cacheline, the IDM which uses the fewest states to encode is applied on the condition that the encoded data size will not exceed the cacheline size. Experimental results show that CRADE can reduce the write energy by 15%, decrease the write latency by 19%, reduce the read latency by 4%, and improve the IPC performance by 2% compared with the state-of-the-art scheme.

**NVM**；**compression**

----

**Encoding Separately: An Energy-Efficient Write Scheme for MLC STT-RAM.** 

> Jie Xu, Dan Feng, Wei Tong, Jingning Liu, Wen Zhou:

Multi Level Cell (**MLC**) Spin Transfer Torque RAM (STT-RAM) provides higher density than Single Level Cell (SLC) STT-RAM by storing two digital bits in a single cell, and is proposed as a promising candidate for on-chip cache. However, MLC STT-RAM suffers from high write energy. We observe that general **encoding** methods, which map the frequent data patterns to the energy-efficient resistance states, cannot reduce the write energy of MLC STT-RAM. <u>To reduce the write energy of MLC STT-RAM</u>, we propose a novel encoding method, i.e., **Encoding Separately** (**ES**). The key idea of ES is to <u>encode the hard bits and soft bits of MLCs separately</u>. The hard bits are encoded for fewer hard-bit writes (hard transitions) and soft bits are encoded for fewer soft-bit writes (soft transitions). Specifically, existing encoding methods commonly used in SLC can be applied to MLC STT-RAM when encoding the two bits separately. We further apply two encoding methods for SLC to MLC STT-RAM through encoding separately, and experimental results show that the proposed scheme can reduce the writes to hard bits and soft bits by 28% and 16%, and achieve an energy reduction of 25%.

MLC;encode

----

**Quick-and-Dirty: Improving Performance of MLC PCM by Using Temporary Short Writes.**

> ​	Mingzhe Zhang, Lunkai Zhang, Lei Jiang, Frederic T. Chong, Zhiyong Liu:

Low write performance is a major obstacle to the commercialization of MLC **PCM**. One opportunity for improving the latency of MLC PCM writes is to <u>use fewer SET iterations in a single write</u>. Unfortunately, the data written by these short writes have significantly shorter retention time and thus need frequent refreshes. As a result, it is impractical to use these short-latency, short-retention writes globally. In this paper, we analyze the temporal behavior of write operations in typical applications and propose **Quick-and-Dirty** (**QnD**), a lightweight scheme to improve the performance of MLC PCM. QnD dynamically performs the short-latency, short-retention write when write operations are bursty, and then uses short-latency, short-retention writes to mitigate the short retention problem when memory system is relatively quiet. Our experimental results show that QnD improves performance by 30.9% on geometric mean while still providing acceptable memory lifetime (7.58 years on geometric mean). We also provide sensitivity studies of the aggressiveness, memory coverage and granularity of QnD technique.

PCM；

## SS2: Effective Voltage Scaling in Late CMOS Era

## SS3: Spin-Computing: Lower the Barrier between Memory and Logic

## S9A: Architecture and Microarchitecture Optimizations

## S9B: Novel Architecture with 3D and Flash Memory

**CooECC: A Cooperative Error Correction Scheme to Reduce LDPC Decoding Latency in NAND Flash.**

> ​	Meng Zhang, Fei Wu, Yajuan Du, Chengmo Yang, Changsheng Xie, Jiguang Wan:

The storage capacity of NAND Flash has increased by scaling down to smaller cell size and using multi-level storage technology, but data reliability is degraded by severer retention errors. To <u>ensure data reliability</u>, error correction codes (ECC) are adopted, such as BCH and low-density parity check (LDPC) codes. However, BCH codes are insufficient when raw bit error rates (RBER) caused by retention errors are high. As a result, BCH codes are inevitably replaced with LDPC codes with stronger error correction capability. Traditional **LDPC** codes are used to independently correct bit errors in the LSB and MSB pages. Unfortunately, decoding latency in such two pages is significantly unbalanced, MSB pages take much higher latency due to higher RBER, leading to suboptimal flash read performance. This paper proposes <u>a cooperative error correction scheme</u>, called **CooECC**, to reduce LDPC decoding latency of the MSB page in NAND Flash. By exploiting data error characteristics introduced by retention errors, CooECC integrates the decoding result of the LSB page into the initial information of LDPC decoding for the MSB page, making it more accurate. This in turn enables decoding to converge at a higher rate. Simulation results show that for LDPC schemes with information lengths of 2KB and 4KB, the decoding latency can be reduced by up to 87% and 84%, respectively, when RBER is as high as 8.0 × 10 -3 .

**LDPC**

----

**Fast, Ring-Based Design of 3D Stacked DRAM.**

> ​	Andrew J. Douglass, Sunil P. Khatri:

As computer memory increases in size and processors continue to get faster, the memory subsystem becomes an increasing bottleneck to system performance. To mitigate the relatively slow DRAM memory chip speeds, a new generation of **3D stacked DRAM** is being developed, with lower power consumption and higher bandwidth. This paper proposes the use of 3D ring-based data fabrics for fast data transfer between these chips. The ring-based data fabric uses a fast standing wave oscillator to clock its transactions. With a fast clocking scheme, and multiple channels sharing the same bus, more channels are utilized while significantly reducing the number of through-silicon vias (TSVs). Experimental results show that our ring-based data fabric can reduce read latencies by almost 4X compared to traditional stacked memory chips. Variations of our scheme can also reduce power consumption compared to traditional memory stacks. Our Memory Architecture using a **Ring-based Scheme** (**MARS**) can effectively trade off power, throughput, and latency to improve system performance for different application spaces. We show that our MARS variants can deliver better latency (up to ~4X), power (up to ~8X), and performance per watt (up to ~4X) over HBM, when averaged over 11 SPEC CPU 2006 benchmarks. Other MARS variants provide higher throughput with similar power consumption compared to Wide I/O memory.

**3D Stacked DRAM**

----

**Memory-Bounded Randomness for Hardware-Constrained Encrypted Computation.**

> ​	Nektarios Georgios Tsoutsos, Oleg Mazonka, Michail Maniatakos:

Encrypted computation enables processing sensitive data directly in the encrypted domain, which allows outsourcing to third parties without compromising privacy. Recent solutions that leverage <u>partial homomorphic encryption</u>, however, require excessive lookup tables or obfuscated software oracles to implement branching over encrypted control values. To address these limitations and make encrypted computations more practical on memory-constrained systems, we present a novel approach for limiting the amount of randomness in probabilistic ciphertexts, using number theory primitives and hash tables. This allows de-randomizing probabilistic ciphertexts and define a new encrypted abstract machine that is memory-friendly to the target system. Compared to obfuscated oracles in previous work, our method performs control flow decisions over ciphertexts twice as fast, while requiring selectively small lookup tables.

----

**Exploiting Process Variation for Read Performance Improvement on LDPC Based Flash Memory Storage Systems.**

> ​	Qiao Li, Liang Shi, Yejia Di, Yajuan Du, Chun Jason Xue, Edwin Hsing-Mean Sha:

With the development of bit density and technology scaling, the process variation (PV) has become much severe on NAND flash memory. As PV presents reliability among flash blocks, which causes read performance variation to read data on different blocks. This paper proposes to improve <u>read performance of LDPC</u> based flash memory by exploiting the reliability characteristics of PV. First, a **block grouping** approach is proposed to classify the flash blocks based on their reliability. Then, **a read data placement scheme** is proposed, which is designed to place read-hot data on flash blocks with high reliability and move read-cold data to blocks with low reliability. Experiment results show that, with negligible overhead, the proposed scheme is able to significantly improve the read performance.

----

**A Design-for-Test Solution for Monolithic 3D Integrated Circuits.** 

> ​	Abhishek Koneru, Sukeshwar Kannan, Krishnendu Chakrabarty

Monolithic three-dimensional (M3D) integration has the potential to achieve significantly higher device density compared to 3D integration based on through-silicon vias (TSVs). We propose a test solution for M3D ICs based on dedicated test layers that are inserted between functional layers. We evaluate the cost associated with the proposed design-for-test (DfT) solution and compare it with that for a potential DfT solution based on the IEEE Std. P1838. Our results show that the proposed solution is more cost-efficient than the P1838-based solution for a wide range of inter-layer via (ILV) density, ILV yield, and defect density.

